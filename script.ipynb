{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from math import floor\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import ravel\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters `original_dataset.json` and removes everything that doesn't `cs.` in it. After that, creates new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('original_dataset.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "filtered = [x for x in data if (' cs.' in x['categories'] or x['categories'][:3] == 'cs.' )]\n",
    "\n",
    "with open('csDataset.json', 'w') as f:\n",
    "    f.write(json.dumps(filtered, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits dataset into two random parts. 80% for training and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csDataset.json', 'r') as f:\n",
    "    data = json.loads(f.read())\n",
    "shuffle(data)\n",
    "length = len(data)\n",
    "train_data_size = floor(length * 80/100)\n",
    "train_data = data[train_data_size:]\n",
    "\n",
    "with open('trainDataset.json', 'w') as f:\n",
    "    f.write(json.dumps(data[:train_data_size], indent=2))\n",
    "\n",
    "with open('testDataset.json', 'w') as f:\n",
    "    f.write(json.dumps(data[train_data_size:], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating topics array and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "d.append(\"cs.AI_Artificial Intelligence\")\n",
    "    \n",
    "d.append(\"cs.CL_Computation and Language\")\n",
    "    \n",
    "d.append(\"cs.CC_Computational Complexity\")\n",
    "    \n",
    "d.append(\"cs.CE_Computational Engineering, Finance, and Science\")\n",
    "    \n",
    "d.append(\"cs.CG_Computational Geometry\")\n",
    "    \n",
    "d.append(\"cs.GT_Computer Science and Game Theory\")\n",
    "    \n",
    "d.append(\"cs.CV_Computer Vision and Pattern Recognition\")\n",
    "    \n",
    "d.append(\"cs.CY_Computers and Society\")\n",
    "\n",
    "d.append(\"cs.CR_Cryptography and Security\")\n",
    "    \n",
    "d.append(\"cs.DS_Data Structures and Algorithms\")\n",
    "    \n",
    "d.append(\"cs.DB_Databases\")\n",
    "    \n",
    "d.append(\"cs.DL_Digital Libraries\")\n",
    "    \n",
    "d.append(\"cs.DM_Discrete Mathematics\")\n",
    "    \n",
    "d.append(\"cs.DC_Distributed, Parallel, and Cluster Computing\")\n",
    "    \n",
    "d.append(\"cs.ET_Emerging Technologies\")\n",
    "    \n",
    "d.append(\"cs.FL_Formal Languages and Automata Theory\")\n",
    "\n",
    "d.append(\"cs.GL_General Literature\")\n",
    "\n",
    "d.append(\"cs.GR_Graphics\")\n",
    "\n",
    "d.append(\"cs.AR_Hardware Architecture\")\n",
    "\n",
    "d.append(\"cs.HC_Human-Computer Interaction\")\n",
    "\n",
    "d.append(\"cs.IR_Information Retrieval\")\n",
    "\n",
    "d.append(\"cs.IT_Information Theory\")\n",
    "\n",
    "d.append(\"cs.LO_Logic in Computer Science\")\n",
    "\n",
    "d.append(\"cs.LG_Machine Learning\")\n",
    "\n",
    "d.append(\"cs.MS_Mathematical Software\")\n",
    "\n",
    "d.append(\"cs.MA_Multiagent Systems\")\n",
    "\n",
    "d.append(\"cs.MM_Multimedia\")\n",
    "\n",
    "d.append(\"cs.NI_Networking and Internet Architecture\")\n",
    "\n",
    "d.append(\"cs.NE_Neural and Evolutionary Computing\")\n",
    "\n",
    "d.append(\"cs.NA_Numerical Analysis\")\n",
    "\n",
    "d.append(\"cs.OS_Operating Systems\")\n",
    "\n",
    "d.append(\"cs.OH_Other Computer Science\")\n",
    "\n",
    "d.append(\"cs.PF_Performance\")\n",
    "\n",
    "d.append(\"cs.PL_Programming Languages\")\n",
    "\n",
    "d.append(\"cs.RO_Robotics\")\n",
    "\n",
    "d.append(\"cs.SI_Social and Information Networks\")\n",
    "\n",
    "d.append(\"cs.SE_Software Engineering\")\n",
    "\n",
    "d.append(\"cs.SD_Sound\")\n",
    "\n",
    "d.append(\"cs.SC_Symbolic Computation\")\n",
    "\n",
    "d.append(\"cs.SY_Systems and Control\")\n",
    "\n",
    "categoriesDict = {x.split('_')[0] : x.split('_')[1] for x in d}\n",
    "\n",
    "categories = [x.split('_')[0] for x in d]\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports `trainJson` and `testJson` files into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainDataset.json', 'r') as f:\n",
    "    trainJson = pd.read_json(f.read())\n",
    "\n",
    "with open('testDataset.json', 'r') as f:\n",
    "    testJson = pd.read_json(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleans and takes keywords from the abstract of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    review = re.sub(r'\\$.*?\\$', '', text)\n",
    "    review = re.sub('[^a-zA-Z-]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    stops = stopwords.words(\"english\")\n",
    "    ps = WordNetLemmatizer()\n",
    "    review = [ps.lemmatize(word) for word in review if not word in set(stops)]\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "\n",
    "def cleaning(trainSet):\n",
    "    corpus = []\n",
    "    for ind, i in trainSet.iterrows():\n",
    "        corpus.append(cleanText(i['title'] + \" \" + i['abstract']))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "corpus = cleaning(trainJson)\n",
    "corpus_test = cleaning(testJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vectors depending on words corpus for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(ngram_range=(1,3),max_df=0.45, max_features=70, stop_words='english', min_df=2)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "\n",
    "cv_test = TfidfVectorizer(ngram_range=(1,3), max_df=0.45, max_features=70, stop_words='english', min_df=2)\n",
    "X_test = cv_test.fit_transform(corpus_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Y` Dataset creation for classlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createY(datas):\n",
    "    y = []\n",
    "    for ind, i in datas.iterrows():\n",
    "        ii = i['categories'].split()\n",
    "        k = False\n",
    "        for j in ii:\n",
    "            if(j[:3] == \"cs.\" and k == False):\n",
    "                y.append(categories.index(j))\n",
    "                k=True\n",
    "    return y\n",
    "y = pd.DataFrame(createY(trainJson))\n",
    "y_test = pd.DataFrame(createY(testJson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{KNeighbors Classification Model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cat = KNeighborsClassifier(n_neighbors=80)\n",
    "\n",
    "cat.fit(X, ravel(y))\n",
    "y_pred = cat.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds Closest Text that is in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The text is about Machine Learning\n",
      "related articles abstracts:\n",
      "\n",
      "Variational Learning with Disentanglement-PyTorch\n",
      "  Unsupervised learning of disentangled representations is an open problem in\n",
      "machine learning. The Disentanglement-PyTorch library is developed to\n",
      "facilitate research, implementation, and testing of new variational algorithms.\n",
      "In this modular library, neural architectures, dimensionality of the latent\n",
      "space, and the training algorithms are fully decoupled, allowing for\n",
      "independent and consistent experiments across variational methods. The library\n",
      "handles the training scheduling, logging, and visualizations of reconstructions\n",
      "and latent space traversals. It also evaluates the encodings based on various\n",
      "disentanglement metrics. The library, so far, includes implementations of the\n",
      "following unsupervised algorithms VAE, Beta-VAE, Factor-VAE, DIP-I-VAE,\n",
      "DIP-II-VAE, Info-VAE, and Beta-TCVAE, as well as conditional approaches such as\n",
      "CVAE and IFCVAE. The library is compatible with the Disentanglement Challenge\n",
      "of NeurIPS 2019, hosted on AICrowd, and achieved the 3rd rank in both the first\n",
      "and second stages of the challenge.\n",
      "\n",
      "   To view full article, use following link: https://arxiv.org/pdf/1912.05184\n",
      "\n",
      "\n",
      "Matrix optimization on universal unitary photonic devices\n",
      "  Universal unitary photonic devices can apply arbitrary unitary\n",
      "transformations to a vector of input modes and provide a promising hardware\n",
      "platform for fast and energy-efficient machine learning using light. We\n",
      "simulate the gradient-based optimization of random unitary matrices on\n",
      "universal photonic devices composed of imperfect tunable interferometers. If\n",
      "device components are initialized uniform-randomly, the locally-interacting\n",
      "nature of the mesh components biases the optimization search space towards\n",
      "banded unitary matrices, limiting convergence to random unitary matrices. We\n",
      "detail a procedure for initializing the device by sampling from the\n",
      "distribution of random unitary matrices and show that this greatly improves\n",
      "convergence speed. We also explore mesh architecture improvements such as\n",
      "adding extra tunable beamsplitters or permuting waveguide layers to further\n",
      "improve the training speed and scalability of these devices.\n",
      "\n",
      "   To view full article, use following link: https://arxiv.org/pdf/1808.00458\n",
      "\n",
      "\n",
      "Efficient training of energy-based models via spin-glass control\n",
      "  We introduce a new family of energy-based probabilistic graphical models for\n",
      "efficient unsupervised learning. Its definition is motivated by the control of\n",
      "the spin-glass properties of the Ising model described by the weights of\n",
      "Boltzmann machines. We use it to learn the Bars and Stripes dataset of various\n",
      "sizes and the MNIST dataset, and show how they quickly achieve the performance\n",
      "offered by standard methods for unsupervised learning. Our results indicate\n",
      "that the standard initialization of Boltzmann machines with random weights\n",
      "equivalent to spin-glass models is an unnecessary bottleneck in the process of\n",
      "training. Furthermore, this new family allows for very easy access to\n",
      "low-energy configurations, which points to new, efficient training algorithms.\n",
      "The simplest variant of such algorithms approximates the negative phase of the\n",
      "log-likelihood gradient with no Markov chain Monte Carlo sampling costs at all,\n",
      "and with an accuracy sufficient to achieve good learning and generalization.\n",
      "\n",
      "   To view full article, use following link: https://arxiv.org/pdf/1910.01592\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from math import log\n",
    "\n",
    "text = \"\"\"\n",
    "In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data for\n",
    "classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,\n",
    "[1] Vapnik et al., 1997[citation needed]) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) \n",
    "and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category\n",
    "or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples\n",
    "to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    " \"\"\"\n",
    "\n",
    "def tfCalculator(arr):\n",
    "    freq={}\n",
    "    \n",
    "    for word in arr:\n",
    "        if(word in freq):\n",
    "            freq[word] += 1\n",
    "        else:\n",
    "            freq[word] = 1\n",
    "    fet_arr = cv.get_feature_names_out()\n",
    "\n",
    "    lena = 1\n",
    "    for i in fet_arr:\n",
    "        if i in arr:\n",
    "            lena+=1\n",
    "\n",
    "    for word in freq:\n",
    "        freq[word] /= lena\n",
    "    return freq\n",
    "\n",
    "def newTFIDFArray(review):\n",
    "    arr = review.split()\n",
    "\n",
    "    freq=tfCalculator(arr)\n",
    "    fet_arr = cv.get_feature_names_out()\n",
    "    new_TFIDF = [0]*len(fet_arr)\n",
    "    \n",
    "    for word in arr: \n",
    "        jima = 1\n",
    "        if(word in fet_arr):\n",
    "            index = np.where(fet_arr == word)[0][0]\n",
    "            \n",
    "            for document in X:\n",
    "                if document[index] != 0:\n",
    "                    jima+=1\n",
    "            \n",
    "            idf = log(len(X)/jima,2)\n",
    "            tfidf = freq[word] * idf\n",
    "            new_TFIDF[index] = tfidf\n",
    "\n",
    "    \n",
    "    return np.array([new_TFIDF])\n",
    "\n",
    "\n",
    "def kneigh(text, n):\n",
    "    print('\\n')\n",
    "    review = cleanText(text)\n",
    "\n",
    "    shemogzavnili = newTFIDFArray(review)\n",
    "    \n",
    "    memezoble = NearestNeighbors(n_neighbors=20).fit(np.append(X, shemogzavnili, axis=0))\n",
    "\n",
    "    answer = memezoble.kneighbors(shemogzavnili, 20, return_distance=False)\n",
    "    predCat = categoriesDict[categories[cat.predict(shemogzavnili)[0]]]\n",
    "    print('\\n'+\"The text is about \" + predCat)\n",
    "    print(\"related articles abstracts:\\n\")\n",
    "    i = 1\n",
    "    while(i < n+1):\n",
    "        if(trainJson.iloc[answer[0][i]]['license'] == None):\n",
    "            continue\n",
    "        if (int(trainJson.iloc[answer[0][i]]['update_date'].split('-')[0]) < 2015):\n",
    "            print(\"Warning, This article may be outdated (\"+trainJson.iloc[answer[0][i]]['update_date'].split('-')[0]+\")\\n\")\n",
    "        print(trainJson.iloc[answer[0][i]]['title'] +'\\n'+ trainJson.iloc[answer[0][i]]['abstract'])\n",
    "        print(\"   To view full article, use following link: \" + \"https://arxiv.org/pdf/\"+trainJson.iloc[answer[0][i]]['id'])\n",
    "        print('\\n')\n",
    "        i+=1\n",
    "        \n",
    "    \n",
    "\n",
    "kneigh(text, 3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
