{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from math import floor\n",
    "from math import log\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy import ravel\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters `original_dataset.json` and removes everything that doesn't `cs.` in it. After that, creates new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('original_dataset.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "filtered = [x for x in data if (' cs.' in x['categories'] or x['categories'][:3] == 'cs.' )]\n",
    "\n",
    "with open('csDataset.json', 'w') as f:\n",
    "    f.write(json.dumps(filtered, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits dataset into two random parts. 80% for training and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csDataset.json', 'r') as f:\n",
    "    data = json.loads(f.read())\n",
    "shuffle(data)\n",
    "length = len(data)\n",
    "train_data_size = floor(length * 80/100)\n",
    "train_data = data[train_data_size:]\n",
    "\n",
    "with open('trainDataset.json', 'w') as f:\n",
    "    f.write(json.dumps(data[:train_data_size], indent=2))\n",
    "\n",
    "with open('testDataset.json', 'w') as f:\n",
    "    f.write(json.dumps(data[train_data_size:], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating topics array and dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "d.append(\"cs.AI_Artificial Intelligence\")\n",
    "    \n",
    "d.append(\"cs.CL_Computation and Language\")\n",
    "    \n",
    "d.append(\"cs.CC_Computational Complexity\")\n",
    "    \n",
    "d.append(\"cs.CE_Computational Engineering, Finance, and Science\")\n",
    "    \n",
    "d.append(\"cs.CG_Computational Geometry\")\n",
    "    \n",
    "d.append(\"cs.GT_Computer Science and Game Theory\")\n",
    "    \n",
    "d.append(\"cs.CV_Computer Vision and Pattern Recognition\")\n",
    "    \n",
    "d.append(\"cs.CY_Computers and Society\")\n",
    "\n",
    "d.append(\"cs.CR_Cryptography and Security\")\n",
    "    \n",
    "d.append(\"cs.DS_Data Structures and Algorithms\")\n",
    "    \n",
    "d.append(\"cs.DB_Databases\")\n",
    "    \n",
    "d.append(\"cs.DL_Digital Libraries\")\n",
    "    \n",
    "d.append(\"cs.DM_Discrete Mathematics\")\n",
    "    \n",
    "d.append(\"cs.DC_Distributed, Parallel, and Cluster Computing\")\n",
    "    \n",
    "d.append(\"cs.ET_Emerging Technologies\")\n",
    "    \n",
    "d.append(\"cs.FL_Formal Languages and Automata Theory\")\n",
    "\n",
    "d.append(\"cs.GL_General Literature\")\n",
    "\n",
    "d.append(\"cs.GR_Graphics\")\n",
    "\n",
    "d.append(\"cs.AR_Hardware Architecture\")\n",
    "\n",
    "d.append(\"cs.HC_Human-Computer Interaction\")\n",
    "\n",
    "d.append(\"cs.IR_Information Retrieval\")\n",
    "\n",
    "d.append(\"cs.IT_Information Theory\")\n",
    "\n",
    "d.append(\"cs.LO_Logic in Computer Science\")\n",
    "\n",
    "d.append(\"cs.LG_Machine Learning\")\n",
    "\n",
    "d.append(\"cs.MS_Mathematical Software\")\n",
    "\n",
    "d.append(\"cs.MA_Multiagent Systems\")\n",
    "\n",
    "d.append(\"cs.MM_Multimedia\")\n",
    "\n",
    "d.append(\"cs.NI_Networking and Internet Architecture\")\n",
    "\n",
    "d.append(\"cs.NE_Neural and Evolutionary Computing\")\n",
    "\n",
    "d.append(\"cs.NA_Numerical Analysis\")\n",
    "\n",
    "d.append(\"cs.OS_Operating Systems\")\n",
    "\n",
    "d.append(\"cs.OH_Other Computer Science\")\n",
    "\n",
    "d.append(\"cs.PF_Performance\")\n",
    "\n",
    "d.append(\"cs.PL_Programming Languages\")\n",
    "\n",
    "d.append(\"cs.RO_Robotics\")\n",
    "\n",
    "d.append(\"cs.SI_Social and Information Networks\")\n",
    "\n",
    "d.append(\"cs.SE_Software Engineering\")\n",
    "\n",
    "d.append(\"cs.SD_Sound\")\n",
    "\n",
    "d.append(\"cs.SC_Symbolic Computation\")\n",
    "\n",
    "d.append(\"cs.SY_Systems and Control\")\n",
    "\n",
    "categoriesDict = {x.split('_')[0] : x.split('_')[1] for x in d}\n",
    "\n",
    "categories = [x.split('_')[0] for x in d]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports `trainJson` and `testJson` files into variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainDataset.json', 'r') as f:\n",
    "    trainJson = pd.read_json(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleans and takes keywords from the abstract of articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    review = re.sub(r'\\$.*?\\$', '', text)\n",
    "    review = re.sub('[^a-zA-Z-]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    stops = stopwords.words(\"english\")\n",
    "    ps = WordNetLemmatizer()\n",
    "    review = [ps.lemmatize(word) for word in review if not word in set(stops)]\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "\n",
    "def cleaning(trainSet):\n",
    "    corpus = []\n",
    "    for ind, i in trainSet.iterrows():\n",
    "        corpus.append(cleanText(i['title'] + \" \" + i['abstract']))\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = cleaning(trainJson)\n",
    "corpus_test = cleaning(testJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.pckl\", 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "with open(\"corpus_test.pckl\", 'rb') as f:\n",
    "    corpus_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vectors depending on words corpus for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [];\n",
    "for i in range(0,40):\n",
    "    cc.append([])\n",
    "\n",
    "for ind, i in trainJson.iterrows():\n",
    "    ii = i['categories'].split()\n",
    "    t = cleanText(i['title'] + \" \" + i['abstract'])\n",
    "    for j in ii:\n",
    "        if(j[:3] == \"cs.\"):\n",
    "            cc[categories.index(j)].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "cvv = []\n",
    "\n",
    "for i in range(0,40):\n",
    "    xx.append([])\n",
    "    cvv.append([])\n",
    "\n",
    "for ind,i in enumerate(cc):\n",
    "    cv = TfidfVectorizer(ngram_range=(1,2),max_df=0.8, max_features=40, stop_words='english', min_df=2)\n",
    "    X = cv.fit_transform(i).toarray()\n",
    "    xx[ind] = X\n",
    "    cvv[ind] = cv\n",
    "\n",
    "for i in cvv:\n",
    "    print(i.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports TFIDFs, X and X_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"x_upd.pckl\",'rb') as f:\n",
    "    xx = pickle.load(f)\n",
    "with open(\"cv_upd.pckl\",'rb') as f:\n",
    "    cvv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Y` Dataset creation for classlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createY(datas):\n",
    "    y = []\n",
    "    for i in range(0,len(datas.index)):\n",
    "        y.append([])\n",
    "    for ind, i in datas.iterrows():\n",
    "        ii = i['categories'].split()\n",
    "        for j in ii:\n",
    "            if(j[:3] == \"cs.\"):\n",
    "                y[ind].append(categories.index(j))\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from math import log\n",
    "\n",
    "text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence - perceiving, synthesizing, and infering information - demonstrated by machines, as opposed to intelligence displayed by animals and humans. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs. OED (OUP) defines artificial intelligence as:[1]\n",
    "\n",
    "the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.\n",
    "\n",
    "AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix),\n",
    "\"\"\"\n",
    "\n",
    "def tfCalculator(arr, cv):\n",
    "    freq={}\n",
    "    \n",
    "    for word in arr:\n",
    "        if(word in freq):\n",
    "            freq[word] += 1\n",
    "        else:\n",
    "            freq[word] = 1\n",
    "    fet_arr = cv.get_feature_names_out()\n",
    "\n",
    "    lena = 1\n",
    "    for i in fet_arr:\n",
    "        if i in arr:\n",
    "            lena+=1\n",
    "\n",
    "    for word in freq:\n",
    "        freq[word] /= lena\n",
    "    return freq\n",
    "\n",
    "def newTFIDFArray(review,cv, X):\n",
    "    arr = review.split()\n",
    "\n",
    "    freq=tfCalculator(arr, cv)\n",
    "    fet_arr = cv.get_feature_names_out()\n",
    "    new_TFIDF = [10]*len(fet_arr)\n",
    "    \n",
    "    for word in arr: \n",
    "        jima = 1\n",
    "        if(word in fet_arr):\n",
    "            index = np.where(fet_arr == word)[0][0]\n",
    "            \n",
    "            for document in X:\n",
    "                if document[index] != 0:\n",
    "                    jima+=1\n",
    "            \n",
    "            idf = log(len(X)/jima,2)\n",
    "            tfidf = freq[word] * idf\n",
    "            new_TFIDF[index] = tfidf\n",
    "\n",
    "    \n",
    "    return np.array([new_TFIDF])\n",
    "\n",
    "\n",
    "def kneigh(text, n, xx, cvv, categories, categoriesDict,trainJson):\n",
    "    m = 1024\n",
    "    c = 0\n",
    "    aaa=[]\n",
    "    for i in range(0,40):\n",
    "        cv = cvv[i]\n",
    "        X = xx[i]\n",
    "        review = cleanText(text)\n",
    "\n",
    "        shemogzavnili = newTFIDFArray(review, cv, X)\n",
    "        \n",
    "        memezoble = NearestNeighbors(n_neighbors=80).fit(np.append(X, shemogzavnili, axis=0))\n",
    "\n",
    "        answer = memezoble.kneighbors(shemogzavnili, 80, return_distance=True)\n",
    "  \n",
    "        if(answer[0][0][1]!=10):\n",
    "            kk = 0\n",
    "            for p in range(1, 80):\n",
    "                kk+=answer[0][0][p]\n",
    "            kk=kk/19\n",
    "\n",
    "            if(m>kk):\n",
    "                m=kk\n",
    "                c = i\n",
    "                aaa = answer[1]\n",
    "    \n",
    "    predCat = categoriesDict[categories[c]]\n",
    "    print('\\n'+\"The text is about \" + predCat)\n",
    "    print(\"related articles abstracts:\\n\")\n",
    "    i = 1\n",
    "    while(i < n+1):\n",
    "        if(trainJson.iloc[aaa[0][i]]['license'] == None):\n",
    "            i+=1\n",
    "            continue\n",
    "        if (int(trainJson.iloc[aaa[0][i]]['update_date'].split('-')[0]) < 2015):\n",
    "            print(\"Warning, This article may be outdated (\"+trainJson.iloc[aaa[0][i]]['update_date'].split('-')[0]+\")\\n\")\n",
    "        print(trainJson.iloc[aaa[0][i]]['title'] +'\\n'+ trainJson.iloc[aaa[0][i]]['abstract'])\n",
    "        print(\"   To view full article, use following link: \" + \"https://arxiv.org/pdf/\"+trainJson.iloc[aaa[0][i]]['id'])\n",
    "        print('\\n')\n",
    "        i+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(texts):\n",
    "    print(\"Imported libraries\")\n",
    "    d = []\n",
    "    d.append(\"cs.AI_Artificial Intelligence\")\n",
    "        \n",
    "    d.append(\"cs.CL_Computation and Language\")\n",
    "        \n",
    "    d.append(\"cs.CC_Computational Complexity\")\n",
    "        \n",
    "    d.append(\"cs.CE_Computational Engineering, Finance, and Science\")\n",
    "        \n",
    "    d.append(\"cs.CG_Computational Geometry\")\n",
    "        \n",
    "    d.append(\"cs.GT_Computer Science and Game Theory\")\n",
    "        \n",
    "    d.append(\"cs.CV_Computer Vision and Pattern Recognition\")\n",
    "        \n",
    "    d.append(\"cs.CY_Computers and Society\")\n",
    "\n",
    "    d.append(\"cs.CR_Cryptography and Security\")\n",
    "        \n",
    "    d.append(\"cs.DS_Data Structures and Algorithms\")\n",
    "        \n",
    "    d.append(\"cs.DB_Databases\")\n",
    "        \n",
    "    d.append(\"cs.DL_Digital Libraries\")\n",
    "        \n",
    "    d.append(\"cs.DM_Discrete Mathematics\")\n",
    "        \n",
    "    d.append(\"cs.DC_Distributed, Parallel, and Cluster Computing\")\n",
    "        \n",
    "    d.append(\"cs.ET_Emerging Technologies\")\n",
    "        \n",
    "    d.append(\"cs.FL_Formal Languages and Automata Theory\")\n",
    "\n",
    "    d.append(\"cs.GL_General Literature\")\n",
    "\n",
    "    d.append(\"cs.GR_Graphics\")\n",
    "\n",
    "    d.append(\"cs.AR_Hardware Architecture\")\n",
    "\n",
    "    d.append(\"cs.HC_Human-Computer Interaction\")\n",
    "\n",
    "    d.append(\"cs.IR_Information Retrieval\")\n",
    "\n",
    "    d.append(\"cs.IT_Information Theory\")\n",
    "\n",
    "    d.append(\"cs.LO_Logic in Computer Science\")\n",
    "\n",
    "    d.append(\"cs.LG_Machine Learning\")\n",
    "\n",
    "    d.append(\"cs.MS_Mathematical Software\")\n",
    "\n",
    "    d.append(\"cs.MA_Multiagent Systems\")\n",
    "\n",
    "    d.append(\"cs.MM_Multimedia\")\n",
    "\n",
    "    d.append(\"cs.NI_Networking and Internet Architecture\")\n",
    "\n",
    "    d.append(\"cs.NE_Neural and Evolutionary Computing\")\n",
    "\n",
    "    d.append(\"cs.NA_Numerical Analysis\")\n",
    "\n",
    "    d.append(\"cs.OS_Operating Systems\")\n",
    "\n",
    "    d.append(\"cs.OH_Other Computer Science\")\n",
    "\n",
    "    d.append(\"cs.PF_Performance\")\n",
    "\n",
    "    d.append(\"cs.PL_Programming Languages\")\n",
    "\n",
    "    d.append(\"cs.RO_Robotics\")\n",
    "\n",
    "    d.append(\"cs.SI_Social and Information Networks\")\n",
    "\n",
    "    d.append(\"cs.SE_Software Engineering\")\n",
    "\n",
    "    d.append(\"cs.SD_Sound\")\n",
    "\n",
    "    d.append(\"cs.SC_Symbolic Computation\")\n",
    "\n",
    "    d.append(\"cs.SY_Systems and Control\")\n",
    "\n",
    "    categoriesDict = {x.split('_')[0] : x.split('_')[1] for x in d}\n",
    "\n",
    "    categories = [x.split('_')[0] for x in d]\n",
    "    with open(\"x_upd.pckl\",'rb') as f:\n",
    "        xx = pickle.load(f)\n",
    "    with open(\"cv_upd.pckl\",'rb') as f:\n",
    "        cvv = pickle.load(f)\n",
    "    print('Almost done importing model...')\n",
    "    with open('trainDataset.json', 'r') as f:\n",
    "        trainJson = pd.read_json(f.read())\n",
    "    print(\"Imported model and started the program\")\n",
    "    ind = 1\n",
    "    for text in texts:\n",
    "        print(\"Text \" + str(ind) +\":\")\n",
    "        kneigh(text, 3, xx, cvv, categories, categoriesDict,trainJson)\n",
    "        ind += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
